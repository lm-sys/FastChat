version: "4.1" #updates for Thomas AI build, set gpus --all

services:
  fastchat-controller:
    # container_name: fastchat-controller
    build:
      context: .
      dockerfile: Dockerfile
    image: fastchat:controller
    ports:
      - "21001:21001"
    restart: "on-failure:5"
    entrypoint: ["python3.8", "-m", "fastchat.serve.controller", "--host", "0.0.0.0", "--port", "21001"]

  fastchat-model-worker:
    container_name: fastchat-model
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
    - path: ./.env
    volumes:
      - huggingface:/root/.cache/huggingface
    image: fastchat:worker
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all  # To utilize all GPUs
            capabilities: [gpu]
    restart: "on-failure:5"
    entrypoint: ["python3.8", "-m", "fastchat.serve.model_worker", "--model-names", "${FASTCHAT_WORKER_MODEL_NAMES:-vicuna-7b-v1.5}", "--model-path", "${FASTCHAT_WORKER_MODEL_PATH:-lmsys/vicuna-7b-v1.3}", "--worker-address", "http://fastchat-model-worker:21002", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21002", "--num-gpus", "2", "--max-gpu-memory", "8GiB"]

  fastchat-gradio-web:
    # container_name: fastchat-web-ui
    build:
      context: .
      dockerfile: Dockerfile
    image: fastchat:web_interface
    ports:
      - "8080:8080"
    restart: "on-failure:5"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # Assuming the connection URL is passed as an argument, but this might vary based on your application's specifics
    entrypoint: ["python3.8", "-m", "fastchat.serve.gradio_web_server_multi", "--controller-url", "http://fastchat-controller:21001"]

  fastchat-api-server:
    # container_name: fastchat-api
    build:
      context: .
      dockerfile: Dockerfile
    image: fastchat:latest
    ports:
      - "8000:8000"
    restart: "on-failure:5"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    entrypoint: ["python3.8", "-m", "fastchat.serve.openai_api_server", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "8000"]

volumes:
  huggingface:
    name: vicuna_7b_volume
