# FastChat for tool
é¡¹ç›®cloneè‡ª https://github.com/lm-sys/FastChat

åœ¨[openai_api_server.py](fastchat%2Fserve%2Fopenai_api_server.py)ä¸­å¢åŠ äº†tools(function)èƒ½åŠ›ï¼Œå¯åœ¨å…¶ä»–éœ€è¦è°ƒç”¨å·¥å…·çš„åœºæ™¯ä¸‹ä½¿ç”¨ã€‚(å¦‚æœå¯¹æ‚¨æœ‰ç”¨ï¼Œéº»çƒ¦ç‚¹ä¸ªstarğŸ™‡â€â™‚ï¸)

å¦‚ langgraphè°ƒç”¨
```python
from langchain_core.pydantic_v1 import SecretStr
from langchain_openai import ChatOpenAI

class LocalChat(ChatOpenAI):
    openai_api_base = "http://127.0.0.1:8002/v1"
    openai_api_key = SecretStr("123456")
    model_name = "Qwen1.5-72B-Chat-GPTQ-Int8"
    temperature = 0.0

prompt = hub.pull("wfh/react-agent-executor")
prompt.pretty_print()

# Choose the LLM that will drive the agent
llm = LocalChat()
agent_executor = create_react_agent(llm, tools, messages_modifier=prompt)

```
## æ”¯æŒæ¨¡å‹
Qwen(å·²æµ‹è¯•)

## ä½¿ç”¨æ–¹å¼
#### Launch the controller
```python
python3 -m fastchat.serve.controller
```
#### Launch the model worker(s)
```python
python3 -m fastchat.serve.model_worker --model-path /models/qwen/Qwen1.5-72B-Chat-GPTQ-Int8
```

#### Launch the API

```python
python -m fastchat.serve.openai_api_server_for_tool --controller-address http://127.0.0.1:21001
```
## å®‰è£…æ–¹å¼

Option1: ç›´æ¥cloneé¡¹ç›®`git clone https://github.com/bluechanel/FastChat`ï¼Œ

å…¶ä»–æ­¥éª¤ https://github.com/lm-sys/FastChat?tab=readme-ov-file#install

Option2: `python -m build` æ‰“åŒ…whlåŒ…åpipå®‰è£…

## å£°æ˜

åªåœ¨Qwen1.5 72B æ¨¡å‹ä¸Šè¿›è¡Œè¿‡æµ‹è¯•ï¼Œå…¶ä»–æ¨¡å‹ä¸ä¿è¯èƒ½å¤Ÿæ­£å¸¸ä½¿ç”¨ã€‚

åŸºäºæ­¤ æœªå‘ FastChat æäº¤PR
